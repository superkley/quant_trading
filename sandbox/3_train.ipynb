{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config> created config from file: './config.json'\n",
      "config> config\n",
      "        - base:\n",
      "            - config_file_path: /mnt/c/notebooks/sandbox/config.json\n",
      "        - datasets:\n",
      "            - stocks: 30\n",
      "            - benchmarks: 69\n",
      "        - prepare:\n",
      "            - data_start_dt: 2018-02-09\n",
      "            - data_end_dt: 2020-01-07\n",
      "            - cache_dir: /mnt/c/notebooks/sandbox/cache/20200107/\n",
      "        - train:            \n",
      "            - window_trading_days: [3, 5, 21, 35, 50]\n",
      "            - lag_trading_days: [1, 2, 3, 4, 5]\n",
      "            - label_max_high_weight: 3.0\n",
      "            - label_max_close_weight: 1.0\n",
      "            - settings: 12\n",
      "        - model:\n",
      "            - max_samples: 40\n",
      "            - batch_size: 200\n",
      "            - lstm_hidden_size: 256\n",
      "            - early_stopping_patience: 10\n",
      "            - validaion_monitor: val_mean_squared_error\n",
      "            - max_epochs: 1000\n",
      "            - base_dir: /mnt/c/notebooks/sandbox/model/20200110/            \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import datetime\n",
    "\n",
    "import shared\n",
    "import config\n",
    "import provider_yfinance as provider\n",
    "\n",
    "reload(shared)\n",
    "reload(config)\n",
    "reload(provider)\n",
    "\n",
    "cfg = config.get_config('^GDAXI')\n",
    "\n",
    "# overwrite download_end_dt: use cached data\n",
    "# config.overwrite_end_dt(cfg, '2019-12-19')\n",
    "# config.save_config(cfg)\n",
    "\n",
    "# model sav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense, BatchNormalization, Masking\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "import pathlib\n",
    "import munch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-nr_1-lookback_3-label_1> training submodel ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 3, 1319)           5276      \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 3, 1319)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 3, 256)            1613824   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 3, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,669,981\n",
      "Trainable params: 2,667,343\n",
      "Non-trainable params: 2,638\n",
      "_________________________________________________________________\n",
      "model> model created\n",
      ":None\n",
      "model> fitting ... (Hit CTRL-C to stop early)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 5333 samples, validate on 593 samples\n",
      "Epoch 1/1000\n",
      "5333/5333 [==============================] - 16s 3ms/step - loss: 1.2713 - mean_absolute_error: 0.7905 - mean_squared_error: 1.2713 - val_loss: 2.4510 - val_mean_absolute_error: 1.0212 - val_mean_squared_error: 2.4510\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 2.45097, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1//best_weights_lstm-256_epoch-01_val-2.4510.hdf5\n",
      "Epoch 2/1000\n",
      "5333/5333 [==============================] - 10s 2ms/step - loss: 0.9576 - mean_absolute_error: 0.6786 - mean_squared_error: 0.9576 - val_loss: 2.3048 - val_mean_absolute_error: 0.9610 - val_mean_squared_error: 2.3048\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 2.45097 to 2.30484, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1//best_weights_lstm-256_epoch-02_val-2.3048.hdf5\n",
      "Epoch 3/1000\n",
      "5333/5333 [==============================] - 8s 2ms/step - loss: 0.9288 - mean_absolute_error: 0.6683 - mean_squared_error: 0.9288 - val_loss: 2.3156 - val_mean_absolute_error: 0.9502 - val_mean_squared_error: 2.3156\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 2.30484\n",
      "Epoch 4/1000\n",
      "5333/5333 [==============================] - 14s 3ms/step - loss: 0.9226 - mean_absolute_error: 0.6607 - mean_squared_error: 0.9226 - val_loss: 2.3584 - val_mean_absolute_error: 0.9796 - val_mean_squared_error: 2.3584\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 2.30484\n",
      "Epoch 5/1000\n",
      "5333/5333 [==============================] - 16s 3ms/step - loss: 0.8886 - mean_absolute_error: 0.6492 - mean_squared_error: 0.8886 - val_loss: 2.2666 - val_mean_absolute_error: 0.9771 - val_mean_squared_error: 2.2666\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 2.30484 to 2.26658, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1//best_weights_lstm-256_epoch-05_val-2.2666.hdf5\n",
      "Epoch 6/1000\n",
      "5333/5333 [==============================] - 16s 3ms/step - loss: 0.8815 - mean_absolute_error: 0.6482 - mean_squared_error: 0.8815 - val_loss: 2.3021 - val_mean_absolute_error: 1.0007 - val_mean_squared_error: 2.3021\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 7/1000\n",
      "5333/5333 [==============================] - 17s 3ms/step - loss: 0.8856 - mean_absolute_error: 0.6470 - mean_squared_error: 0.8856 - val_loss: 2.3528 - val_mean_absolute_error: 0.9883 - val_mean_squared_error: 2.3528\n",
      "\n",
      "Epoch 00007: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 8/1000\n",
      "5333/5333 [==============================] - 32s 6ms/step - loss: 0.8458 - mean_absolute_error: 0.6285 - mean_squared_error: 0.8458 - val_loss: 2.4605 - val_mean_absolute_error: 1.0473 - val_mean_squared_error: 2.4605\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 9/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.8370 - mean_absolute_error: 0.6311 - mean_squared_error: 0.8370 - val_loss: 2.2783 - val_mean_absolute_error: 0.9762 - val_mean_squared_error: 2.2783\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 10/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.8262 - mean_absolute_error: 0.6219 - mean_squared_error: 0.8262 - val_loss: 2.3807 - val_mean_absolute_error: 1.0156 - val_mean_squared_error: 2.3807\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 11/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.8014 - mean_absolute_error: 0.6090 - mean_squared_error: 0.8014 - val_loss: 2.3942 - val_mean_absolute_error: 1.0070 - val_mean_squared_error: 2.3942\n",
      "\n",
      "Epoch 00011: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 12/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.7861 - mean_absolute_error: 0.6083 - mean_squared_error: 0.7861 - val_loss: 2.4617 - val_mean_absolute_error: 1.0448 - val_mean_squared_error: 2.4617\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 13/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.7762 - mean_absolute_error: 0.6041 - mean_squared_error: 0.7762 - val_loss: 2.5401 - val_mean_absolute_error: 1.0446 - val_mean_squared_error: 2.5401\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 14/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.7560 - mean_absolute_error: 0.6017 - mean_squared_error: 0.7560 - val_loss: 2.4761 - val_mean_absolute_error: 1.0485 - val_mean_squared_error: 2.4761\n",
      "\n",
      "Epoch 00014: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 15/1000\n",
      "5333/5333 [==============================] - 9s 2ms/step - loss: 0.7184 - mean_absolute_error: 0.5819 - mean_squared_error: 0.7184 - val_loss: 2.5244 - val_mean_absolute_error: 1.0272 - val_mean_squared_error: 2.5244\n",
      "\n",
      "Epoch 00015: val_mean_squared_error did not improve from 2.26658\n",
      "Epoch 00015: early stopping\n",
      "model> trying to save weights ...\n",
      "model> saved model weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/model_weights.hdf5'\n",
      "model> saved optimizer weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/optimizer_weights.pkl'\n",
      "sm-nr_1-lookback_3-label_1> overall-val_mean_squared_error (best epoch): 2.2665849182296163\n",
      "sm-nr_1-lookback_3-label_1> overall-val_mean_squared_error (+-5 around best epoch): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 3, 1319)           5276      \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, 3, 1319)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 3, 256)            1613824   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 3, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,669,981\n",
      "Trainable params: 2,667,343\n",
      "Non-trainable params: 2,638\n",
      "_________________________________________________________________\n",
      "model> model created\n",
      ":None\n",
      "model> fitting ... (Hit CTRL-C to stop early)\n",
      "Train on 179 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "179/179 [==============================] - 9s 48ms/step - loss: 2.8272 - mean_absolute_error: 1.2483 - mean_squared_error: 2.8272 - val_loss: 4.1426 - val_mean_absolute_error: 1.7885 - val_mean_squared_error: 4.1426\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 4.14260, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/1COV.DE/best_weights_lstm-256_epoch-01_val-4.1426.hdf5\n",
      "Epoch 2/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 2.6703 - mean_absolute_error: 1.2053 - mean_squared_error: 2.6703 - val_loss: 4.1533 - val_mean_absolute_error: 1.8011 - val_mean_squared_error: 4.1533\n",
      "\n",
      "Epoch 00002: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 3/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 2.4644 - mean_absolute_error: 1.1512 - mean_squared_error: 2.4644 - val_loss: 4.1704 - val_mean_absolute_error: 1.8198 - val_mean_squared_error: 4.1704\n",
      "\n",
      "Epoch 00003: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 4/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 2.1630 - mean_absolute_error: 1.0622 - mean_squared_error: 2.1630 - val_loss: 4.2040 - val_mean_absolute_error: 1.8476 - val_mean_squared_error: 4.2040\n",
      "\n",
      "Epoch 00004: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 5/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.7780 - mean_absolute_error: 0.9374 - mean_squared_error: 1.7780 - val_loss: 4.2434 - val_mean_absolute_error: 1.8791 - val_mean_squared_error: 4.2434\n",
      "\n",
      "Epoch 00005: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 6/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.3768 - mean_absolute_error: 0.8056 - mean_squared_error: 1.3768 - val_loss: 4.2769 - val_mean_absolute_error: 1.9077 - val_mean_squared_error: 4.2769\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 7/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.0125 - mean_absolute_error: 0.6890 - mean_squared_error: 1.0125 - val_loss: 4.3290 - val_mean_absolute_error: 1.9288 - val_mean_squared_error: 4.3290\n",
      "\n",
      "Epoch 00007: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 8/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.8304 - mean_absolute_error: 0.6620 - mean_squared_error: 0.8304 - val_loss: 4.3807 - val_mean_absolute_error: 1.9209 - val_mean_squared_error: 4.3807\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 9/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.8122 - mean_absolute_error: 0.6970 - mean_squared_error: 0.8122 - val_loss: 4.4079 - val_mean_absolute_error: 1.8721 - val_mean_squared_error: 4.4079\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 10/1000\n",
      "179/179 [==============================] - 1s 3ms/step - loss: 0.8308 - mean_absolute_error: 0.7122 - mean_squared_error: 0.8308 - val_loss: 4.4223 - val_mean_absolute_error: 1.7841 - val_mean_squared_error: 4.4223\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 11/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.7253 - mean_absolute_error: 0.6538 - mean_squared_error: 0.7253 - val_loss: 4.4681 - val_mean_absolute_error: 1.8357 - val_mean_squared_error: 4.4681\n",
      "\n",
      "Epoch 00011: val_mean_squared_error did not improve from 4.14260\n",
      "Epoch 00011: early stopping\n",
      "model> trying to save weights ...\n",
      "model> saved model weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/1COV.DE/model_weights.hdf5'\n",
      "model> saved optimizer weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/1COV.DE/optimizer_weights.pkl'\n",
      "sm-nr_1-lookback_3-label_1> 1COV.DE-val_mean_squared_error (best epoch): 4.142602920532227\n",
      "sm-nr_1-lookback_3-label_1> 1COV.DE-val_mean_squared_error (+-5 around best epoch): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 3, 1319)           5276      \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, 3, 1319)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 3, 256)            1613824   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 3, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,669,981\n",
      "Trainable params: 2,667,343\n",
      "Non-trainable params: 2,638\n",
      "_________________________________________________________________\n",
      "model> model created\n",
      ":None\n",
      "model> fitting ... (Hit CTRL-C to stop early)\n",
      "Train on 179 samples, validate on 20 samples\n",
      "Epoch 1/1000\n",
      "179/179 [==============================] - 5s 29ms/step - loss: 1.5984 - mean_absolute_error: 0.9781 - mean_squared_error: 1.5984 - val_loss: 2.0482 - val_mean_absolute_error: 1.1171 - val_mean_squared_error: 2.0482\n",
      "\n",
      "Epoch 00001: val_mean_squared_error improved from inf to 2.04820, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/ADS.DE/best_weights_lstm-256_epoch-01_val-2.0482.hdf5\n",
      "Epoch 2/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.4888 - mean_absolute_error: 0.9399 - mean_squared_error: 1.4888 - val_loss: 2.0260 - val_mean_absolute_error: 1.1076 - val_mean_squared_error: 2.0260\n",
      "\n",
      "Epoch 00002: val_mean_squared_error improved from 2.04820 to 2.02603, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/ADS.DE/best_weights_lstm-256_epoch-02_val-2.0260.hdf5\n",
      "Epoch 3/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.3354 - mean_absolute_error: 0.8828 - mean_squared_error: 1.3354 - val_loss: 2.0075 - val_mean_absolute_error: 1.0955 - val_mean_squared_error: 2.0075\n",
      "\n",
      "Epoch 00003: val_mean_squared_error improved from 2.02603 to 2.00748, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/ADS.DE/best_weights_lstm-256_epoch-03_val-2.0075.hdf5\n",
      "Epoch 4/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 1.1509 - mean_absolute_error: 0.8098 - mean_squared_error: 1.1509 - val_loss: 1.9960 - val_mean_absolute_error: 1.0778 - val_mean_squared_error: 1.9960\n",
      "\n",
      "Epoch 00004: val_mean_squared_error improved from 2.00748 to 1.99600, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/ADS.DE/best_weights_lstm-256_epoch-04_val-1.9960.hdf5\n",
      "Epoch 5/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.8867 - mean_absolute_error: 0.7008 - mean_squared_error: 0.8867 - val_loss: 1.9867 - val_mean_absolute_error: 1.0497 - val_mean_squared_error: 1.9867\n",
      "\n",
      "Epoch 00005: val_mean_squared_error improved from 1.99600 to 1.98672, saving model to /mnt/c/notebooks/sandbox/model/20200110//nr_1-lookback_3-label_1/ADS.DE/best_weights_lstm-256_epoch-05_val-1.9867.hdf5\n",
      "Epoch 6/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.6724 - mean_absolute_error: 0.5854 - mean_squared_error: 0.6724 - val_loss: 2.0043 - val_mean_absolute_error: 1.0988 - val_mean_squared_error: 2.0043\n",
      "\n",
      "Epoch 00006: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 7/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5259 - mean_absolute_error: 0.5179 - mean_squared_error: 0.5259 - val_loss: 2.0868 - val_mean_absolute_error: 1.1476 - val_mean_squared_error: 2.0868\n",
      "\n",
      "Epoch 00007: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 8/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5217 - mean_absolute_error: 0.5634 - mean_squared_error: 0.5217 - val_loss: 2.1840 - val_mean_absolute_error: 1.1636 - val_mean_squared_error: 2.1840\n",
      "\n",
      "Epoch 00008: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 9/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.5130 - mean_absolute_error: 0.5888 - mean_squared_error: 0.5130 - val_loss: 2.2218 - val_mean_absolute_error: 1.1373 - val_mean_squared_error: 2.2218\n",
      "\n",
      "Epoch 00009: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 10/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.4341 - mean_absolute_error: 0.5364 - mean_squared_error: 0.4341 - val_loss: 2.1954 - val_mean_absolute_error: 1.1049 - val_mean_squared_error: 2.1954\n",
      "\n",
      "Epoch 00010: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 11/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.3125 - mean_absolute_error: 0.4163 - mean_squared_error: 0.3125 - val_loss: 2.1241 - val_mean_absolute_error: 1.0772 - val_mean_squared_error: 2.1241\n",
      "\n",
      "Epoch 00011: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 12/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.2738 - mean_absolute_error: 0.3787 - mean_squared_error: 0.2738 - val_loss: 2.0708 - val_mean_absolute_error: 1.0691 - val_mean_squared_error: 2.0708\n",
      "\n",
      "Epoch 00012: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 13/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.2865 - mean_absolute_error: 0.3865 - mean_squared_error: 0.2865 - val_loss: 2.1076 - val_mean_absolute_error: 1.0454 - val_mean_squared_error: 2.1076\n",
      "\n",
      "Epoch 00013: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 14/1000\n",
      "179/179 [==============================] - 0s 2ms/step - loss: 0.2554 - mean_absolute_error: 0.3683 - mean_squared_error: 0.2554 - val_loss: 2.1948 - val_mean_absolute_error: 1.0722 - val_mean_squared_error: 2.1948\n",
      "\n",
      "Epoch 00014: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 15/1000\n",
      "179/179 [==============================] - 0s 3ms/step - loss: 0.2152 - mean_absolute_error: 0.3204 - mean_squared_error: 0.2152 - val_loss: 2.2788 - val_mean_absolute_error: 1.0972 - val_mean_squared_error: 2.2788\n",
      "\n",
      "Epoch 00015: val_mean_squared_error did not improve from 1.98672\n",
      "Epoch 00015: early stopping\n",
      "model> trying to save weights ...\n",
      "model> saved model weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/ADS.DE/model_weights.hdf5'\n",
      "model> saved optimizer weights to '/mnt/c/notebooks/sandbox/model/20200110/nr_1-lookback_3-label_1/ADS.DE/optimizer_weights.pkl'\n",
      "sm-nr_1-lookback_3-label_1> ADS.DE-val_mean_squared_error (best epoch): 1.986720323562622\n",
      "sm-nr_1-lookback_3-label_1> ADS.DE-val_mean_squared_error (+-5 around best epoch): nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_4 (Batch (None, 3, 1319)           5276      \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, 3, 1319)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 3, 256)            1613824   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 3, 256)            525312    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,669,981\n",
      "Trainable params: 2,667,343\n",
      "Non-trainable params: 2,638\n",
      "_________________________________________________________________\n",
      "model> model created\n",
      ":None\n",
      "model> fitting ... (Hit CTRL-C to stop early)\n"
     ]
    }
   ],
   "source": [
    "def load_weights(cfg, submodel_settings, model, ticker_name=''):\n",
    "    pth_submodel = pathlib.Path(f\"{cfg.model.base_dir}/{submodel_settings.id}/{ticker_name}\")\n",
    "    f_model_weights = pth_submodel.joinpath(cfg.model.model_weights_file_name)\n",
    "    f_optimizer_weights = pth_submodel.joinpath(cfg.model.optimizer_weights_file_name)\n",
    "    shared.mkdirs(pth_submodel)\n",
    "    if f_model_weights.is_file():\n",
    "        try:\n",
    "            model.load_weights(f_model_weights)\n",
    "            print(f\"model> loaded model weights from '{f_model_weights.resolve()}'\")\n",
    "        except ValueError as e:\n",
    "            print(f\"WARN model> failed to load model weights from '{f_model_weights.resolve()}': ${e}\")\n",
    "    if f_optimizer_weights.is_file():\n",
    "        model._make_train_function()\n",
    "        try:\n",
    "            with open(f_optimizer_weights.resolve(), 'rb') as f:\n",
    "                model.optimizer.set_weights(pickle.load(f))    \n",
    "                print(f\"model> loaded optimizer weights from '{f_optimizer_weights.resolve()}'\")\n",
    "        except ValueError as e:\n",
    "            print(f\"WARN model> failed to load optimizer weights from '{f_optimizer_weights.resolve()}': ${e}\")\n",
    "    \n",
    "def save_weights(cfg, submodel_settings, model, ticker_name=''):\n",
    "    print(f\"model> trying to save weights ...\") \n",
    "    pth_submodel = pathlib.Path(f\"{cfg.model.base_dir}/{submodel_settings.id}/{ticker_name}\")\n",
    "    f_model_weights = pth_submodel.joinpath(cfg.model.model_weights_file_name)\n",
    "    f_optimizer_weights = pth_submodel.joinpath(cfg.model.optimizer_weights_file_name)\n",
    "    shared.mkdirs(pth_submodel)\n",
    "    model.save_weights(f_model_weights)\n",
    "    print(f\"model> saved model weights to '{f_model_weights.resolve()}'\")\n",
    "    with open(f_optimizer_weights.resolve(), 'wb') as f:\n",
    "        pickle.dump(K.batch_get_value(getattr(model.optimizer, 'weights')), f)\n",
    "        print(f\"model> saved optimizer weights to '{f_optimizer_weights.resolve()}'\")\n",
    "\n",
    "def create_model(cfg, submodel_settings, model_data, ticker_name=''):\n",
    "    num_samples = model_data.shape[0]\n",
    "    num_features = len(model_data.X.head(1).tolist()[0][0][0][0])\n",
    "    input_length = submodel_settings.lookback_days\n",
    "    input_dim = num_features\n",
    "    lstm_dim = cfg.model.lstm_hidden_size\n",
    "    output_dim = 1\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(input_length, input_dim)))\n",
    "    model.add(Masking())    \n",
    "    model.add(LSTM(lstm_dim, dropout=.2, recurrent_dropout=.2, return_sequences=True, activation=\"softsign\"))\n",
    "    model.add(LSTM(lstm_dim, dropout=.2, recurrent_dropout=.2, return_sequences=True, activation=\"softsign\"))\n",
    "    model.add(LSTM(lstm_dim, dropout=.2, recurrent_dropout=.2, activation=\"softsign\"))\n",
    "    model.add(Dense(output_dim))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    print(f'model> model created\\n:{model.summary()}')\n",
    "    load_weights(cfg, submodel_settings, model, ticker_name)\n",
    "    return model\n",
    "\n",
    "def train_model(cfg, submodel_settings, model, model_data, ticker_name=''):\n",
    "    num_samples = model_data.shape[0]\n",
    "    num_features = len(model_data.X.head(1).tolist()[0][0][0][0])\n",
    "    input_length = submodel_settings.lookback_days\n",
    "    input_dim = num_features    \n",
    "    output_dim = 1\n",
    "    X = np.hstack(np.asarray(model_data.X)).reshape(num_samples, input_length, input_dim)\n",
    "    y = np.hstack(np.asarray(model_data.y)).reshape(num_samples, output_dim)\n",
    "    pth_submodel = f\"{cfg.model.base_dir}/{submodel_settings.id}/{ticker_name}\"\n",
    "    shared.mkdirs(pth_submodel)\n",
    "    monitor = cfg.model.validaion_monitor\n",
    "    patience = cfg.model.early_stopping_patience\n",
    "    fit_params = {\n",
    "        \"batch_size\": cfg.model.batch_size,\n",
    "        \"epochs\": cfg.model.max_epochs,\n",
    "        \"verbose\": 1,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"shuffle\": True,\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(verbose=True, patience=patience, monitor=monitor),\n",
    "            ModelCheckpoint(f\"{pth_submodel}/best_weights_lstm-{cfg.model.lstm_hidden_size}_epoch-{{epoch:02d}}_val-{{{monitor}:.4f}}.hdf5\", monitor=monitor, verbose=1, save_best_only=True)\n",
    "        ]\n",
    "    }\n",
    "    print('model> fitting ... (Hit CTRL-C to stop early)')\n",
    "    history = None\n",
    "    try:\n",
    "        history = model.fit(X, y, **fit_params)\n",
    "    except KeyboardInterrupt:\n",
    "        print('model> training stopped early!')\n",
    "        history = model.history        \n",
    "    save_weights(cfg, submodel_settings, model, ticker_name)\n",
    "    return history\n",
    "\n",
    "monitor = cfg.model.validaion_monitor\n",
    "patience = cfg.model.early_stopping_patience\n",
    "for submodel_settings in cfg.train.settings:\n",
    "    print(f\"sm-{submodel_settings.id}> training submodel ...\")\n",
    "    model_data = provider.prepare_submodel_data(cfg, submodel_settings)\n",
    "    model = create_model(cfg, submodel_settings, model_data)\n",
    "    history = train_model(cfg, submodel_settings, model, model_data)\n",
    "    print(f\"sm-{submodel_settings.id}> overall-{monitor} (best epoch): {history.history[monitor][np.max(history.epoch)-patience]}\")\n",
    "    print(f\"sm-{submodel_settings.id}> overall-{monitor} (+-5 around best epoch): {np.mean(history.history[monitor][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])}\")\n",
    "    for ticker_name in model_data.ticker.unique().tolist():\n",
    "        ticker_data = model_data[model_data.ticker==ticker_name]\n",
    "        model = create_model(cfg, submodel_settings, ticker_data, ticker_name)\n",
    "        history = train_model(cfg, submodel_settings, model, ticker_data, ticker_name)\n",
    "        print(f\"sm-{submodel_settings.id}> {ticker_name}-{monitor} (best epoch): {history.history[monitor][np.max(history.epoch)-patience]}\")\n",
    "        print(f\"sm-{submodel_settings.id}> {ticker_name}-{monitor} (+-5 around best epoch): {np.mean(history.history[monitor][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])}\")        \n",
    "        \n",
    "# single stock\n",
    "# all stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model> {monitor}(+-5 around best epoch): {np.mean(history.history[monitor][(np.max(history.epoch)-patience-5):(np.max(history.epoch)-patience+5)])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({\n",
    "    'X': pd.Series(model.predict(X_train).flatten()),\n",
    "    'y': y_train.flatten()\n",
    "})\n",
    "df_test['diff'] = df_test.X - df_test.y\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/39674713/neural-network-lstm-input-shape-from-dataframe\n",
    "https://stackoverflow.com/questions/49803503/lstm-preprocessing-build-3d-arrays-from-pandas-data-frame-based-on-id"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
